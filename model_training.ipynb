{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14936aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, average_precision_score, brier_score_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import joblib\n",
    "\n",
    "ART = Path(\"artifacts\")\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d94d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(ART / \"meta.json\") as f:\n",
    "    targets = json.load(f)[\"targets\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b08fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(task, split):\n",
    "    df = pd.read_csv(ART/f\"{task}_{split}.csv\")\n",
    "    y = df[targets[task]]\n",
    "    X = df.drop(columns=[targets[task]])\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a90bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_records(X, task):\n",
    "    # Prepare for DictVectorizer\n",
    "    return [{c: r[c] for c in X.columns if pd.notna(r[c])} | {f\"task={task}\": 1} for _, r in X.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f85699e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_task(task):\n",
    "    X, y = load_split(task, \"train\")\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    Xv = vec.fit_transform(to_records(X, task))\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
    "    lr_grid = {\"C\": np.logspace(-3, 2, 15), \"penalty\": [\"l2\"], \"solver\": [\"lbfgs\", \"liblinear\"]}\n",
    "    lr_rs = RandomizedSearchCV(lr, lr_grid, n_iter=12, scoring=scorer, cv=skf, random_state=RANDOM_SEED, n_jobs=-1).fit(Xv, y)\n",
    "\n",
    "    # GradientBoostingClassifier\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb_grid = {\"n_estimators\": [100,200,300], \"learning_rate\": [0.03,0.1,0.2], \"max_depth\": [2,3,4], \"subsample\": [0.7,0.85,1.0]}\n",
    "    gb_rs = RandomizedSearchCV(gb, gb_grid, n_iter=12, scoring=scorer, cv=skf, random_state=RANDOM_SEED, n_jobs=-1).fit(Xv, y)\n",
    "\n",
    "    joblib.dump(vec, ART / f\"{task}_tune_vectorizer.joblib\")\n",
    "    print(f\"{task.upper()} best LR AUC={lr_rs.best_score_:.3f}, best params={lr_rs.best_params_}\")\n",
    "    print(f\"{task.upper()} best GB AUC={gb_rs.best_score_:.3f}, best params={gb_rs.best_params_}\")\n",
    "\n",
    "    return {\n",
    "        \"task\": task,\n",
    "        \"best\": {\n",
    "            \"lr\": {\"params\": lr_rs.best_params_, \"auc\": float(lr_rs.best_score_)},\n",
    "            \"gb\": {\"params\": gb_rs.best_params_, \"auc\": float(gb_rs.best_score_)}\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbf37425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAST best LR AUC=nan, best params={'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(43.93970560760795)}\n",
      "BREAST best GB AUC=nan, best params={'subsample': 0.7, 'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIABETES best LR AUC=nan, best params={'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(43.93970560760795)}\n",
      "DIABETES best GB AUC=nan, best params={'subsample': 0.7, 'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEART best LR AUC=nan, best params={'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(43.93970560760795)}\n",
      "HEART best GB AUC=nan, best params={'subsample': 0.7, 'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIDNEY best LR AUC=nan, best params={'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(43.93970560760795)}\n",
      "KIDNEY best GB AUC=nan, best params={'subsample': 0.7, 'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "tasks = [\"breast\", \"diabetes\", \"heart\", \"kidney\"]\n",
    "tuning_results = [tune_task(t) for t in tasks]\n",
    "with open(ART / \"tuning_results.json\", \"w\") as f:\n",
    "    json.dump(tuning_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465fb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(ART / \"tuning_results.json\") as f:\n",
    "    tuning = {d[\"task\"]: d[\"best\"] for d in json.load(f)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f7f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def train_final_model(task):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    import joblib\n",
    "\n",
    "    Xtr, ytr = load_split(task, \"train\")\n",
    "    Xca, yca = load_split(task, \"calib\")\n",
    "    Xte, yte = load_split(task, \"test\")\n",
    "    vec = DictVectorizer(sparse=False)\n",
    "    XTR = vec.fit_transform(to_records(Xtr, task))\n",
    "    XCA = vec.transform(to_records(Xca, task))\n",
    "    XTE = vec.transform(to_records(Xte, task))\n",
    "\n",
    "    best_lr = tuning[task][\"lr\"]\n",
    "    best_gb = tuning[task][\"gb\"]\n",
    "    use_gb = best_gb[\"auc\"] >= best_lr[\"auc\"]\n",
    "    if use_gb:\n",
    "        model = GradientBoostingClassifier(**best_gb[\"params\"])\n",
    "    else:\n",
    "        model = LogisticRegression(max_iter=5000, class_weight=\"balanced\", **best_lr[\"params\"])\n",
    "\n",
    "    model.fit(XTR, ytr)\n",
    "    cal = CalibratedClassifierCV(model, method=\"sigmoid\", cv=\"prefit\").fit(XCA, yca)\n",
    "    joblib.dump(vec, ART/f\"{task}_vectorizer.joblib\")\n",
    "    joblib.dump(cal, ART/f\"{task}_calibrated_model.joblib\")\n",
    "    print(f\"Saved {task} vectorizer and calibrated model.\")\n",
    "    return cal, vec, XTE, yte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39a24441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved breast vectorizer and calibrated model.\n",
      "\n",
      "Results for BREAST:\n",
      "AUROC: 0.9636243386243386\n",
      "AUPRC: 0.9455649565573653\n",
      "Brier score: 0.07532242871944982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved diabetes vectorizer and calibrated model.\n",
      "\n",
      "Results for DIABETES:\n",
      "AUROC: 0.8229629629629629\n",
      "AUPRC: 0.7016708705458802\n",
      "Brier score: 0.16656441618101236\n",
      "Saved heart vectorizer and calibrated model.\n",
      "\n",
      "Results for HEART:\n",
      "AUROC: 0.7832167832167832\n",
      "AUPRC: 0.8373265623265623\n",
      "Brier score: 0.2066517883864368\n",
      "Saved kidney vectorizer and calibrated model.\n",
      "\n",
      "Results for KIDNEY:\n",
      "AUROC: 1.0\n",
      "AUPRC: 1.0\n",
      "Brier score: 0.02803798150797554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EXTECH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    cal, vec, XTE, yte = train_final_model(task)\n",
    "    p_te = cal.predict_proba(XTE)[:,1]\n",
    "    print(f\"\\nResults for {task.upper()}:\")\n",
    "    print(\"AUROC:\", roc_auc_score(yte, p_te))\n",
    "    print(\"AUPRC:\", average_precision_score(yte, p_te))\n",
    "    print(\"Brier score:\", brier_score_loss(yte, p_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02b5141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast risk band (90%): qhat=0.412\n",
      "diabetes risk band (90%): qhat=0.707\n",
      "heart risk band (90%): qhat=0.611\n",
      "kidney risk band (90%): qhat=0.158\n"
     ]
    }
   ],
   "source": [
    "import math, numpy as np\n",
    "\n",
    "def compute_risk_band_threshold(cal, XCA, yca, alpha=0.1):\n",
    "    p_ca = cal.predict_proba(XCA)[:,1]\n",
    "    p_true = np.where(yca==1, p_ca, 1 - p_ca)\n",
    "    k = int(math.ceil((len(p_true)+1)*(1-alpha)))\n",
    "    qhat = float(np.partition(1 - p_true, k-1)[k-1])\n",
    "    return qhat\n",
    "\n",
    "for task in tasks:\n",
    "    Xtr, ytr = load_split(task, \"train\")\n",
    "    Xca, yca = load_split(task, \"calib\")\n",
    "    vec = joblib.load(ART/f\"{task}_vectorizer.joblib\")\n",
    "    cal = joblib.load(ART/f\"{task}_calibrated_model.joblib\")\n",
    "    XCA = vec.transform(to_records(Xca, task))\n",
    "    qhat = compute_risk_band_threshold(cal, XCA, yca)\n",
    "    with open(ART/f\"{task}_conformal.json\",\"w\") as f:\n",
    "        json.dump({\"alpha\":0.1,\"qhat\":qhat}, f)\n",
    "    print(f\"{task} risk band (90%): qhat={qhat:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
